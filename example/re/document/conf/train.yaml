adam_epsilon: 1e-06
bert_lr: 3e-05
channel_type: 'context-based'
config_name: ''
data_dir: 'data'
dataset: 'docred'
dev_file: 'dev.json'
down_dim: 256
evaluation_steps: -1
gradient_accumulation_steps: 2
learning_rate: 0.0004
log_dir: './train_roberta.log'
max_grad_norm: 1.0
max_height: 42
max_seq_length: 1024
model_name_or_path: 'roberta-base'
num_class: 97
num_labels: 4
num_train_epochs: 30
save_path: './model_roberta.pt'
seed: 111
test_batch_size: 2
test_file: 'test.json'
tokenizer_name: ''
train_batch_size: 2
train_file: 'train_annotated.json'
train_from_saved_model: ''
transformer_type: 'roberta'
unet_in_dim: 3
unet_out_dim: 256
warmup_ratio: 0.06
load_path: './model_roberta.pt'